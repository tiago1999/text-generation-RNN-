{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems = pd.read_csv('/Users/tiagoornelas/Documents/Ironhack/Projects/Final project/scraped_rnn.csv')\n",
    "\n",
    "poems = poems.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding unique values per poem \n",
    "\n",
    "def unique_values (row):\n",
    "\n",
    "    return sorted(set(row))\n",
    "\n",
    "poems['unique'] = poems['Poem'].apply(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating set of unique characters \n",
    "\n",
    "vocab_list = list(poems['unique'])\n",
    "\n",
    "unique_list = []\n",
    "\n",
    "for i in vocab_list:\n",
    "    for x in i:\n",
    "        unique_list.append(x)\n",
    "\n",
    "unique_vocab = sorted(set(unique_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "298"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-28 13:06:10.061698: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "## vectorizing text\n",
    "\n",
    "def char_encode (row):\n",
    "\n",
    "    return tf.strings.unicode_split(row, input_encoding='UTF-8')\n",
    "\n",
    "char_encode_df = poems['Poem'].apply(char_encode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting tokens to character ID's\n",
    "\n",
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(unique_vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectors_to_char_id (row):\n",
    "\n",
    "    return ids_from_chars(tf.strings.unicode_split(row, 'UTF-8'))\n",
    "\n",
    "all_ids = poems['Poem'].apply(vectors_to_char_id)\n",
    "\n",
    "\n",
    "all_ids_df_list = list(all_ids)\n",
    "all_ids_list = []\n",
    "\n",
    "for i in all_ids_df_list:\n",
    "    for x in i:\n",
    "        all_ids_list.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids in ids_dataset.take(60):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'D' b'o' b'g' b' ' b'b' b'o' b'n' b'e' b',' b' ' b's' b't' b'a' b'p'\n",
      " b'l' b'e' b'r' b',' b' ' b'c' b'r' b'i' b'b' b'b' b'a' b'g' b'e' b' '\n",
      " b'b' b'o' b'a' b'r' b'd' b',' b' ' b'g' b'a' b'r' b'l' b'i' b'c' b' '\n",
      " b'p' b'r' b'e' b's' b's' b' ' b' ' b' ' b' ' b' ' b' ' b'b' b'e' b'c'\n",
      " b'a' b'u' b's' b'e' b' ' b't' b'h' b'i' b's' b' ' b'w' b'i' b'n' b'd'\n",
      " b'o' b'w' b' ' b'i' b's' b' ' b'l' b'o' b'o' b's' b'e' b'\\xe2\\x80\\x94'\n",
      " b'l' b'a' b'c' b'k' b's' b' ' b's' b'u' b'c' b't' b'i' b'o' b'n' b','\n",
      " b' ' b'l' b'a' b'c' b'k'], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Dog bone, stapler, cribbage board, garlic press      because this window is loose\\xe2\\x80\\x94lacks suction, lack'\n",
      "b's grip. Bungee cord, bootstrap, dog leash, leather belt      because this window had sash cords. They'\n",
      "b\" frayed. They broke. Feather duster, thatch of straw, empty bottle of Elmer's glue      because this \"\n",
      "b'window is loud\\xe2\\x80\\x94its hinges clack open, clack shut. Stuffed bear, baby blanket, single crib newel      '\n",
      "b\"because this window is split. It's dividing in two. Velvet moss, sagebrush, willow branch, robin's wi\"\n"
     ]
    }
   ],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
    "\n",
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'Dog bone, stapler, cribbage board, garlic press      because this window is loose\\xe2\\x80\\x94lacks suction, lac'\n",
      "Target: b'og bone, stapler, cribbage board, garlic press      because this window is loose\\xe2\\x80\\x94lacks suction, lack'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tiagoornelas/Documents/Ironhack/Projects/Final project/model/rnn_poems.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tiagoornelas/Documents/Ironhack/Projects/Final%20project/model/rnn_poems.ipynb#ch0000018?line=0'>1</a>\u001b[0m BATCH_SIZE \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tiagoornelas/Documents/Ironhack/Projects/Final%20project/model/rnn_poems.ipynb#ch0000018?line=1'>2</a>\u001b[0m BUFFER_SIZE \u001b[39m=\u001b[39m \u001b[39m10000\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tiagoornelas/Documents/Ironhack/Projects/Final%20project/model/rnn_poems.ipynb#ch0000018?line=3'>4</a>\u001b[0m dataset \u001b[39m=\u001b[39m (\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tiagoornelas/Documents/Ironhack/Projects/Final%20project/model/rnn_poems.ipynb#ch0000018?line=4'>5</a>\u001b[0m     dataset\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tiagoornelas/Documents/Ironhack/Projects/Final%20project/model/rnn_poems.ipynb#ch0000018?line=5'>6</a>\u001b[0m     \u001b[39m.\u001b[39mshuffle(BUFFER_SIZE)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tiagoornelas/Documents/Ironhack/Projects/Final%20project/model/rnn_poems.ipynb#ch0000018?line=6'>7</a>\u001b[0m     \u001b[39m.\u001b[39mbatch(BATCH_SIZE, drop_remainder\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tiagoornelas/Documents/Ironhack/Projects/Final%20project/model/rnn_poems.ipynb#ch0000018?line=7'>8</a>\u001b[0m     \u001b[39m.\u001b[39mprefetch(tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mAUTOTUNE))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 299) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'he Pacific Ocean watching the sea stretch past a gauze of power lines into a green horizon this summ'\n",
      "\n",
      "Next Char Predictions:\n",
      " b'=\\xc3\\xbf\\xc3\\xa0\\xe7\\xbe\\x8eQ\\xc3\\xb4\\xce\\xbe\\xe1\\xba\\xa3\\xc3\\xb6\\xe7\\x8c\\xaa\\xc3\\x86S\\xc3\\x89\\xc2\\xb4+\\xce\\xb7\\xc3\\xaclc\\xce\\xac\\xce\\xad\\xe2\\x94\\x80:\\xc3\\xa1\"\\xc5\\xbb\\xce\\xb3F\\xe2\\x80\\xa2&]\\xc5\\xbd6X\\xe1\\xbb\\x91\\xce\\xa3\\xcf\\x8c}\\xe5\\xb9\\xbf\\xc3\\xb1*\\xc4\\x93S\\xcf\\x89\\xe2\\x80\\x82\\xe6\\x97\\xa6\\xc3\\x94\\xe2\\x89\\x88\\xc5\\xafi\\xe2\\x94\\x80T\\xe2\\x80\\xa2\\xe1\\xbb\\xa3\\xc3\\x9a\\xc3\\x88\\xc3\\x9f\\xce\\xad\\xe2\\x80\\xa2\\xce\\xaf\\xcb\\x9a\\xe5\\xb9\\xbfN\\xe1\\xbb\\x91\\xe1\\xbb\\x89\\xe2\\x80\\x9d\\xce\\xbe\\xe2\\x80\\x94p.W\\xe7\\x9b\\xae~\\xcf\\x80\\xc3\\x8b\\xc3\\xad\\xe1\\xba\\xa5$Az\\xc3\\x9ac\\xe6\\x9c\\xaa\\xcc\\xa7\\xcc\\x84k\\xc3\\x88\\xc3\\x93\\xc3\\xb9\\xc4\\x9f\\xc3\\xa1m+\\xc5\\x93\\xc4\\xaby\\xc4\\x9f\\xc4\\x9b\\xc2\\xa3P'\n"
     ]
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "\n",
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 299)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(5.7031765, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299.81827"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compiling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving checkpoints\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3096/3096 [==============================] - 7860s 3s/step - loss: 1.7139\n",
      "Epoch 2/10\n",
      "3096/3096 [==============================] - 8243s 3s/step - loss: 1.4605\n",
      "Epoch 3/10\n",
      "3096/3096 [==============================] - 7594s 2s/step - loss: 1.4217\n",
      "Epoch 4/10\n",
      "3096/3096 [==============================] - 8188s 3s/step - loss: 1.4017\n",
      "Epoch 5/10\n",
      "3096/3096 [==============================] - 6377s 2s/step - loss: 1.3894\n",
      "Epoch 6/10\n",
      "3096/3096 [==============================] - 6353s 2s/step - loss: 1.3812\n",
      "Epoch 7/10\n",
      "3096/3096 [==============================] - 6380s 2s/step - loss: 1.3752\n",
      "Epoch 8/10\n",
      "3096/3096 [==============================] - 6729s 2s/step - loss: 1.3711\n",
      "Epoch 9/10\n",
      "3096/3096 [==============================] - 7673s 2s/step - loss: 1.3681\n",
      "Epoch 10/10\n",
      "3096/3096 [==============================] - 7459s 2s/step - loss: 1.3658\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.OneStep at 0x1790ec940>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_step_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love, less You kill well if you lie —                               so dream'd wrong,With grave, gazing, forbeam, and men municipl'd Sictries— Things fall apart, serameauble void.      Those branches, however care to see The God and wine, not one night of food; and the great men's long city, and the cold,          And the makes me wander nest, Meet your noses, and dearer and hair, We might art. I’m always on harvesting                Dark mother, in that purpose and two Survivors to a king's moment. Serven    is the deif night versoful after shear.                                                             on                a cold shore, a tone  standing at grappery          beaches frozen                   into the hull,    odds one of those human eye:       later that after death of philosophie used again.     Brown and the only lost interfusions    they died and go away.   Sweet nim crazed by now your difference,  both me cheer       in my hand. I think if it were fulfilled —I see    s \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 6.907888174057007\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['Love'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x280ec0640>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ironclasses')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bfe8cd4d37db90c8b9468f14ce817f9cd906576255a34a1d0cefacef45ef3b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
